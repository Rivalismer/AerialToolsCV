{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38018baa-4e63-4461-8973-36f35b02c064",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "This notebook serves as the basis for training the model to recognise and classify solar panels. \n",
    "\n",
    "## Questions\n",
    "- How will the drone fly? Can I expect it to have a top-down view from the camera?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe2f535-70c6-4750-b8c1-b01c3d083a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import utils\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from skimage import io, data\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb, rgb2gray, gray2rgb\n",
    "from engine import train_one_epoch, evaluate\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4c2ab-18a5-4375-98d0-f3875ff8378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading the dataset\n",
    "# One thing worth noting is that this dataset is entirely comprised of top-down pictures, so could perform\n",
    "# more poorly from other angles\n",
    "train_path = \"../Data/train/\"\n",
    "img = \"../Data/train/img/2021_02_03_10_A_90DJI_0032_height_30m.JPG\"\n",
    "\n",
    "# Get image size\n",
    "img_org = io.imread(img)\n",
    "print(f\"Image size {img_org.shape}\")\n",
    "\n",
    "# Ensure all images are of the same size and change type to tensors (which is the torch equivalent of np-arrays)\n",
    "# It might be worth resizing them smaller, but there is a tradeoff between computational speed and accuracy\n",
    "# In the end, perhaps the best solution is to use the size which the images comes from the camera\n",
    "train_transforms = T.Compose([T.Resize((2250, 4000)), T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91e35d6-1fc7-405f-ad14-006d4e39562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganise data set into labels and images - you only need to run this once after downloading data set!\n",
    "\n",
    "# Define paths\n",
    "train_path = \"../Data/train_val_set/\"\n",
    "new_train_img = \"../Data/train/img/\"\n",
    "test_path = \"../Data/test_set/\"\n",
    "new_test_img = \"../Data/test/img/\"\n",
    "\n",
    "# Get images\n",
    "img_train = [f for f in os.listdir(train_path) if '.jpg' in f.lower()]\n",
    "img_test = [f for f in os.listdir(test_path) if '.jpg' in f.lower()]\n",
    "\n",
    "# Change folders\n",
    "for img in img_train:\n",
    "    old_path = train_path + img\n",
    "    img_path = new_train_img + img\n",
    "    shutil.move(old_path, img_path)\n",
    "\n",
    "for img in img_test:\n",
    "    old_path = test_path + img\n",
    "    img_path = new_test_img + img\n",
    "    shutil.move(old_path, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61c42d9-a00d-4703-b0e6-23cc5d310f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our own dataset\n",
    "class PVDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, d_transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = d_transforms\n",
    "        \n",
    "        # Load all image files\n",
    "        self.imgs = list(os.listdir(os.path.join(root, \"img/\")))\n",
    "        self.masks = list(os.listdir(os.path.join(root, \"labels/\")))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images and masks\n",
    "        img_path = os.path.join(self.root, \"img/\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"labels/\", self.masks[idx])\n",
    "        \n",
    "        # Convert the image to RGB\n",
    "        #img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        # Convert PIL to np-array\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # BLOB Analysis\n",
    "        label_im = label(mask)\n",
    "        \n",
    "        # Instances are different colours\n",
    "        obj_ids = np.unique(label_im)\n",
    "        \n",
    "        # First id is background - remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        # Split into multiple separate mask segments\n",
    "        masks = (label_im[:, None, None] == obj_ids[:, None, None])\n",
    "        \n",
    "        # Loop through and get the boxes\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(label_im == obj_ids[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are individual\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb49f8-9585-45be-ae7a-f726aed1bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datasets - Note that ImageFolder requires each class of interest to be its own subfolder\n",
    "#train_ds = torchvision.datasets.ImageFolder(root = train_path, transform = train_transforms)\n",
    "train_ds = PVDataSet(\"../Data/train/\", train_transforms)\n",
    "\n",
    "# Load set, make sure not to shuffle since the labelled images are listed 1 entry after the original\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_ds, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbb9e0-b484-4895-9a64-ebdee7a0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and standard deviation (might be useful later)\n",
    "def get_mean_and_std(loader):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total_img = 0\n",
    "    \n",
    "    # Iterate through batches, we don't need their names, hence _\n",
    "    for batch, _ in loader:\n",
    "        # How many images are there (could be less than 32)\n",
    "        img_count = batch.size(0)\n",
    "        \n",
    "        # Get the actual images out of the batch, reshape them\n",
    "        images = batch.view(img_count, batch.size(1), -1)\n",
    "        \n",
    "        # Add mean/std from batch to total\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_img += img_count\n",
    "    \n",
    "    # Get mean and std\n",
    "    mean /= total_img\n",
    "    std /= total_img\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5bbbd-8c40-42f7-a38e-fef955ef7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and std\n",
    "mean, std = get_mean_and_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20018f-d01a-416a-8062-1d436b5fb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {mean}, Standard Deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602bef29-aaf6-4075-9ba4-302fadcc22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to use the gpu for training\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5208a521-42ec-47cd-96af-5088a119b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    device = set_device()\n",
    "    \n",
    "    # Train through all the epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch number %d\" % (epoch + 1))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        # Run optimizer on all batches (this might be changed if\n",
    "        # there is a better algorithm than SGD)\n",
    "        for data in train_loader.dataset:\n",
    "            # Send data to gpu\n",
    "            images, labels, _, _ = data\n",
    "            images = images.to(device)\n",
    "            labels = images.to(device)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Set gradient to 0 so parameters update correctly\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images)\n",
    "            \n",
    "            # Get the prediction for output\n",
    "            _, predictions = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Check how many items were identified incorrectly\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Back-propagate\n",
    "            loss.backwards()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update loss & correct\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "            \n",
    "        # Print the result of the epoch\n",
    "        epoch_loss = running_loss/len(train_loader)\n",
    "        epoch_acc = 100.00 * running_correct / total\n",
    "        \n",
    "        print(\"   - Training dataset got %d out of %d images correctly (%.3f##). Epoch loss: %.3f\"\n",
    "              % (running_correct, total, epoch_acc, epoch))\n",
    "        \n",
    "        # Test model\n",
    "        evaluate_model(model, test_loader)\n",
    "        \n",
    "    print(\"Finished\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1152c9c2-d434-49de-801c-31ffee073d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    # Notify all layers that you are evaluating - such that dropoff applies\n",
    "    model.eval()\n",
    "    pred_correct = 0\n",
    "    total = 0\n",
    "    device = set_device()\n",
    "    \n",
    "    # Activate no_grad, such that we don't backpropagate during evaluation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Same as training, except without updating weights\n",
    "        for data in test_loader.dataset:\n",
    "            images, labels, _, _ = data\n",
    "            images = images.to(device)\n",
    "            labels = images.to(device)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Get outputs and find predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Update counts\n",
    "            pred_correct += (predicted == labels).sum().item() \n",
    "    \n",
    "    # Update and print epoch accuracy\n",
    "    epoch_acc = 100.00 * running_correct / total\n",
    "    print(\"   - Training dataset got %d out of %d images correctly (%.3f##). Epoch loss: %.3f\"\n",
    "          % (running_correct, total, epoch_acc, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb9ede9-1a6e-4d9a-84e1-708202b8b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load model on pre-trained COCO\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Get number of inputs for classifier\n",
    "    in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace pre-trained head with new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "    \n",
    "    # Get input features for the mask classifier\n",
    "    in_feat_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # Replace predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_feat_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6af1ae-c63f-4fbd-865e-09cc3065ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    mean = [0.3014, 0.2905, 0.2335]\n",
    "    std = [0.0938, 0.0904, 0.0874]\n",
    "\n",
    "    # This train transforms could also have elements randomly flipped, but the problem is that the \n",
    "    # labelled image would then be wrong - thus introducing errors into our model\n",
    "    d_transforms = []\n",
    "    #d_transforms.append(trans.Resize(2250, 4000))\n",
    "    d_transforms.append(T.PILToTensor())\n",
    "    #d_transforms.append(trans.Normalize(torch.Tensor(mean), torch.Tensor(std)))\n",
    "    d_transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    return T.Compose(d_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb863b5-8eaa-4923-ae94-b1372b61b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - using pretrained means it already knows the classification of 1000 classes defined\n",
    "# in the ImageNet database\n",
    "# The model is here chosen to be resnet 18, but might be changed for resnet 50 in the future for a faster R-CNN network.\n",
    "# Note that this current model is NOT an R-CNN, which might mean it's not very good currently since we are looking for\n",
    "# the region(s) of the picture in which the item is located, not a classification of the image as a whole\n",
    "\n",
    "# Remake the transforms, now with normalization using the acquired mean and std\n",
    "train_path = \"../Data/train/\"\n",
    "test_path = \"../Data/test/\"\n",
    "\n",
    "# Mean and std for static segmentation\n",
    "\n",
    "# Mean and std for moving_labeled\n",
    "#mean = [0.2723, 0.2529, 0.2091]\n",
    "#std = [0.1044, 0.0988, 0.0860]\n",
    "\n",
    "# Make our datasets\n",
    "train_ds = PVDataSet(train_path, get_transform())\n",
    "test_ds = PVDataSet(test_path, get_transform())\n",
    "\n",
    "# Put them into loaders\n",
    "# Collate_fn ensures correct data padding\n",
    "# Num_workers speeds up the process by allowing paralell processing\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_ds, batch_size = 32, \n",
    "                                           shuffle = False, collate_fn=utils.collate_fn) #num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_ds, batch_size = 32, \n",
    "                                          shuffle = False, collate_fn=utils.collate_fn) #num_workers=2)\n",
    "\n",
    "# Define how many new classes we would like to learn - here it is only solar panels for the moment\n",
    "num_classes = 2 # Solar panels + background\n",
    "\n",
    "# Prepare matrices for forward propagation using the number of classes and features\n",
    "#resnet18_model.fc = nn.Linear(num_features, num_classes)\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = set_device()\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Define the loss function (between 0.001 - 0.1) & optimizer (Stochastic Gradient Descent)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params, lr = 0.01, momentum = 0.9, weight_decay = 0.003)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6883e2db-21d3-490f-a589-6fd2a767fb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Train one epoch, print every 10 iterations\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 31\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:83\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m     original_image_sizes\u001b[38;5;241m.\u001b[39mappend((val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m---> 83\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Check for degenerate boxes\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# TODO: Move this to a function\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:130\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image)\n\u001b[1;32m--> 130\u001b[0m image, target_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m images[i] \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:181\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.resize\u001b[1;34m(self, image, target)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# FIXME assume for now that testing uses the largest scale\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_size[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 181\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[43m_resize_image_and_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:67\u001b[0m, in \u001b[0;36m_resize_image_and_masks\u001b[1;34m(image, self_min_size, self_max_size, target, fixed_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m target:\n\u001b[0;32m     66\u001b[0m     mask \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 67\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbyte()\n\u001b[0;32m     70\u001b[0m     target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mask\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3961\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3961\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   3962\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3963\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3964\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), mode)\n\u001b[0;32m   3965\u001b[0m )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)"
     ]
    }
   ],
   "source": [
    "# Train the nn\n",
    "#resnet18_model = train_nn(resnet18_model, train_loader, test_loader, loss_fn, optimizer, 10)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train one epoch, print every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq = 10)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluate(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09e750-a062-4905-86b2-d86a15cb7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../Data/train/img/2021_02_03_10_A_90DJI_0032_height_30m.JPG\"\n",
    "mask_path = \"../Data/train/labels/2021_02_03_10_A_90DJI_0032_height_30m.png\"\n",
    "\n",
    "# Attempt with cv2\n",
    "# im = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# params = cv2.SimpleBlobDetector_Params()\n",
    "# Disable unwanted filter criteria params\n",
    "# params.filterByInertia = False\n",
    "# params.filterByConvexity = False\n",
    "# params.filterByCircularity = False\n",
    "\n",
    "# blob_detector = cv2.SimpleBlobDetector(params)\n",
    "# points = blob_detector.detect(im)\n",
    "\n",
    "# imshow(\"Blobs\", points)\n",
    "\n",
    "# Attempt with skimage\n",
    "# im = io.imread(mask_path)\n",
    "\n",
    "mask = cv2.open(mask_path)\n",
    "\n",
    "# Convert PIL to np-array\n",
    "mask = np.array(mask)\n",
    "label_im = label(mask)\n",
    "vals = np.unique(label_im)\n",
    "vals = vals[1:]\n",
    "#print(vals[:, None, None])\n",
    "masks = (label_im[:, None, None] == vals[:, None, None])\n",
    "#print(masks)\n",
    "num_objs = len(vals)\n",
    "boxes = []\n",
    "for i in range(len(vals)):\n",
    "    pos = np.where(label_im == vals[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])\n",
    "print(boxes)\n",
    "\n",
    "# convert everything into a torch.Tensor\n",
    "boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# there is only one class\n",
    "labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "image_id = torch.tensor(0)\n",
    "area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "# suppose all instances are individual\n",
    "iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "target = {}\n",
    "target[\"boxes\"] = boxes\n",
    "target[\"labels\"] = labels\n",
    "target[\"masks\"] = masks\n",
    "target[\"image_id\"] = image_id\n",
    "target[\"area\"] = area\n",
    "target[\"iscrowd\"] = iscrowd\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a28e16-f187-423c-bcc4-834cab20211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"../Data/train/labels/2021_02_03_10_A_90DJI_0032_height_30m.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423841c4-3986-4acb-8499-f3e54e29d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader.dataset.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cf41c-71fe-4505-a2af-7e15035bd557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8f917d62fd31e6967f17dd1bde2862c49dbf86620e52c2a8ea7c8ce5b1f3d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
