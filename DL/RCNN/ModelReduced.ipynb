{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d69528c-84fa-4ff9-8975-9fa66db9c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import utils\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import io, data\n",
    "from skimage.measure import label\n",
    "from skimage.color import label2rgb, rgb2gray, gray2rgb\n",
    "from engine import train_one_epoch, evaluate\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ad5c12-019e-4a39-82c6-2e237369c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our own dataset\n",
    "class PVDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, d_transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = d_transforms\n",
    "        \n",
    "        # Load all image files\n",
    "        #self.imgs = list(os.listdir(os.path.join(root, \"img/\")))\n",
    "        #self.masks = list(os.listdir(os.path.join(root, \"labels/\")))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images and masks\n",
    "        img_path = os.path.join(self.root, \"img/\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"labels/\", self.masks[idx])\n",
    "        \n",
    "        # Convert the image to RGB and resize\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = ImageOps.grayscale(mask)\n",
    "        \n",
    "        # Convert PIL to np-array\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # BLOB Analysis\n",
    "        label_im = label(mask)\n",
    "        \n",
    "        # Instances are different colours\n",
    "        obj_ids = np.unique(label_im)\n",
    "        \n",
    "        # First id is background - remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        # Split into multiple separate mask segments\n",
    "        masks = (label_im[:, None, None] == obj_ids[:, None, None])\n",
    "        \n",
    "        # Convert masks to C, D, H, W\n",
    "        masks = np.transpose(masks, (2,1,0,3))\n",
    "        \n",
    "        # Loop through and get the boxes\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(label_im == obj_ids[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # There is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # Suppose all instances are individual\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c630f8-18d7-4c76-b51e-e8f6d04172b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to use the gpu for training\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881aa356-2b36-4aae-b0c7-7293947ff120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load model on pre-trained COCO\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT', min_size = 2000, max_size = 2500)\n",
    "    \n",
    "    # Get number of inputs for classifier\n",
    "    in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace pre-trained head with new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "    \n",
    "    # Get input features for the mask classifier\n",
    "    in_feat_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # Replace predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_feat_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a445cc-9f53-49ac-8acb-b7be41373b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    # Don't need std and mean as this is handled internally by the model\n",
    "    d_transforms = []\n",
    "    #d_transforms.append(T.Resize([1125,2000]))\n",
    "    d_transforms.append(T.PILToTensor())\n",
    "    d_transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        d_transforms.append(T.RandomHorizontalFlip())\n",
    "    return T.Compose(d_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38956387-b91c-41a0-a09e-e2a7e6069a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(2000,), max_size=2500, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
      "    (mask_head): MaskRCNNHeads(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_predictor): MaskRCNNPredictor(\n",
      "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build model - using pretrained means it already knows the classification of 1000 classes defined\n",
    "# in the ImageNet database\n",
    "# The model is here chosen to be resnet 18, but might be changed for resnet 50 in the future for a faster R-CNN network.\n",
    "# Note that this current model is NOT an R-CNN, which might mean it's not very good currently since we are looking for\n",
    "# the region(s) of the picture in which the item is located, not a classification of the image as a whole\n",
    "\n",
    "# Remake the transforms, now with normalization using the acquired mean and std\n",
    "train_path = \"../Data/train/\"\n",
    "test_path = \"../Data/test/\"\n",
    "\n",
    "# Make our datasets\n",
    "train_ds = PVDataSet(train_path, get_transform(train=True))\n",
    "test_ds = PVDataSet(test_path, get_transform(train=False))\n",
    "\n",
    "# Put them into loaders\n",
    "# Collate_fn ensures correct data padding\n",
    "# Num_workers speeds up the process by allowing paralell processing\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_ds, batch_size = 16, \n",
    "                                           shuffle = True, collate_fn=utils.collate_fn, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_ds, batch_size = 16, \n",
    "                                          shuffle = False, collate_fn=utils.collate_fn, num_workers=4)\n",
    "\n",
    "# Define how many new classes we would like to learn - here it is only solar panels for the moment\n",
    "num_classes = 2 # Solar panels + background\n",
    "\n",
    "# Prepare matrices for forward propagation using the number of classes and features\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "print(model)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = set_device()\n",
    "# device = \"cpu\"\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Define the loss function (between 0.001 - 0.1) & optimizer (Stochastic Gradient Descent)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params, lr = 0.001, momentum = 0.9, weight_decay = 0.003)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07909293-8e57-4e3f-b7aa-34278f850533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\askeo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/15]  eta: 0:40:40  lr: 0.000072  loss: 13.4542 (13.4542)  loss_classifier: 0.8433 (0.8433)  loss_box_reg: 0.0137 (0.0137)  loss_mask: 4.9172 (4.9172)  loss_objectness: 6.7789 (6.7789)  loss_rpn_box_reg: 0.9011 (0.9011)  time: 162.6883  data: 14.7286\n",
      "Epoch: [0]  [ 3/15]  eta: 0:31:56  lr: 0.000286  loss: 10.7365 (10.7424)  loss_classifier: 0.7992 (0.7913)  loss_box_reg: 0.0296 (0.0345)  loss_mask: 3.8813 (3.7565)  loss_objectness: 5.2630 (5.3577)  loss_rpn_box_reg: 0.7334 (0.8023)  time: 159.6875  data: 13.4980\n",
      "Epoch: [0]  [ 6/15]  eta: 0:23:28  lr: 0.000501  loss: 7.5312 (7.3714)  loss_classifier: 0.6768 (0.6202)  loss_box_reg: 0.0356 (0.0389)  loss_mask: 2.1384 (2.3535)  loss_objectness: 3.9631 (3.6791)  loss_rpn_box_reg: 0.7233 (0.6797)  time: 156.5542  data: 13.0269\n",
      "Epoch: [0]  [ 9/15]  eta: 0:15:32  lr: 0.000715  loss: 2.3490 (5.7171)  loss_classifier: 0.3724 (0.4752)  loss_box_reg: 0.0299 (0.0349)  loss_mask: 0.3082 (1.6519)  loss_objectness: 1.3712 (2.9352)  loss_rpn_box_reg: 0.5256 (0.6198)  time: 155.3947  data: 12.9067\n",
      "Epoch: [0]  [12/15]  eta: 0:07:38  lr: 0.000929  loss: 1.9470 (4.7683)  loss_classifier: 0.2751 (0.3830)  loss_box_reg: 0.0299 (0.0326)  loss_mask: 0.0894 (1.2709)  loss_objectness: 1.3412 (2.4970)  loss_rpn_box_reg: 0.5046 (0.5847)  time: 152.7375  data: 12.7258\n",
      "Epoch: [0]  [14/15]  eta: 0:02:29  lr: 0.001000  loss: 1.9300 (4.2808)  loss_classifier: 0.1857 (0.3444)  loss_box_reg: 0.0299 (0.0324)  loss_mask: 0.0313 (1.1015)  loss_objectness: 1.2623 (2.2355)  loss_rpn_box_reg: 0.4912 (0.5670)  time: 149.7490  data: 12.5478\n",
      "Epoch: [0] Total time: 0:37:26 (149.7557 s / it)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ndarray is not Fortran contiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\engine.py:85\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m metric_logger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mMetricLogger(delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 85\u001b[0m coco \u001b[38;5;241m=\u001b[39m \u001b[43mget_coco_api_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m iou_types \u001b[38;5;241m=\u001b[39m _get_iou_types(model)\n\u001b[0;32m     87\u001b[0m coco_evaluator \u001b[38;5;241m=\u001b[39m CocoEvaluator(coco, iou_types)\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\coco_utils.py:207\u001b[0m, in \u001b[0;36mget_coco_api_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCocoDetection):\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcoco\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_coco_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\coco_utils.py:187\u001b[0m, in \u001b[0;36mconvert_to_coco_api\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m    185\u001b[0m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ann_id\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[1;32m--> 187\u001b[0m     ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcoco_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[0;32m    189\u001b[0m     ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m keypoints[i]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pycocotools\\mask.py:85\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(bimask)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bimask\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     84\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m bimask\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbimask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mpycocotools\\_mask.pyx:136\u001b[0m, in \u001b[0;36mpycocotools._mask.encode\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ndarray is not Fortran contiguous"
     ]
    }
   ],
   "source": [
    "# Train the nn\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train one epoch, print every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq = 3)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluate(model, test_loader, device=device)\n",
    "\n",
    "# Currently doesn't train correctly, could perhaps have to do with the rescaling function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71366b23-7778-4124-a38a-36ff829cc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and make plots\n",
    "# pick one image from the test set\n",
    "img, _ = test_ds[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971e021-cd45-4820-9671-dc767f86b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192f125-7464-4964-b3fd-3c8ce22adb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94386c-a447-4631-85c7-ce18bb0501fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dd099-74dc-4b10-a8ae-507c8776307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "img_path = \"../Data/train/img/2021_02_03_10_A_90DJI_0032_height_30m.JPG\"\n",
    "mask_path = \"../Data/train/labels/2021_02_03_10_A_90DJI_0032_height_30m.png\"\n",
    "\n",
    "# Convert the image to RGB and resize\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "w, h = img.size\n",
    "img = img.resize((w//2, h//2))\n",
    "\n",
    "# Convert to grayscale and resize\n",
    "mask = Image.open(mask_path)\n",
    "mask = ImageOps.grayscale(mask)\n",
    "w, h = mask.size\n",
    "mask = mask.resize((w//2, h//2))\n",
    "\n",
    "# Convert PIL to np-array\n",
    "mask = np.array(mask)\n",
    "label_im = label(mask)\n",
    "vals = np.unique(label_im)\n",
    "vals = vals[1:]\n",
    "masks = (label_im[:, None, None] == vals[:, None, None])\n",
    "masks = np.transpose(masks, (2,1,0,3))\n",
    "\n",
    "num_objs = len(vals)\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(label_im == vals[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])\n",
    "#print(boxes)\n",
    "\n",
    "\n",
    "# convert everything into a torch.Tensor\n",
    "boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# there is only one class\n",
    "labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "image_id = torch.tensor(0)\n",
    "area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "# suppose all instances are individual\n",
    "iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "print(np.shape(masks))\n",
    "\n",
    "masks[0,:,:,:].permute(0,2,1)\n",
    "print(np.shape(masks[0,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fbee6-35cf-41bd-b4fe-956c383018b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8f917d62fd31e6967f17dd1bde2862c49dbf86620e52c2a8ea7c8ce5b1f3d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
