{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d69528c-84fa-4ff9-8975-9fa66db9c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import utils\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import io, data\n",
    "from skimage.measure import label\n",
    "from skimage.color import label2rgb, rgb2gray, gray2rgb\n",
    "from engine import train_one_epoch, evaluate\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ad5c12-019e-4a39-82c6-2e237369c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our own dataset\n",
    "class PVDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, d_transforms = None):\n",
    "        self.root = root\n",
    "        self.transforms = d_transforms\n",
    "        \n",
    "        # Load all image files\n",
    "        self.imgs = list(os.listdir(os.path.join(root, \"img/\")))\n",
    "        self.masks = list(os.listdir(os.path.join(root, \"labels/\")))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images and masks\n",
    "        img_path = os.path.join(self.root, \"img/\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"labels/\", self.masks[idx])\n",
    "        \n",
    "        # Convert the image to RGB and resize\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #w, h = img.size\n",
    "        #img = img.resize((w//2, h//2))\n",
    "        \n",
    "        # Convert to grayscale and resize\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = ImageOps.grayscale(mask)\n",
    "        #w, h = mask.size\n",
    "        #mask = mask.resize((w//2, h//2))\n",
    "        \n",
    "        # Convert PIL to np-array\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # BLOB Analysis\n",
    "        label_im = label(mask)\n",
    "        \n",
    "        # Instances are different colours\n",
    "        obj_ids = np.unique(label_im)\n",
    "        \n",
    "        # First id is background - remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        # Split into multiple separate mask segments\n",
    "        masks = (label_im[:, None, None] == obj_ids[:, None, None])\n",
    "        \n",
    "        # Convert masks to C, D, H, W\n",
    "        masks = np.transpose(masks, (2,1,0,3))\n",
    "        \n",
    "        # Loop through and get the boxes\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(label_im == obj_ids[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # There is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # Suppose all instances are individual\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c630f8-18d7-4c76-b51e-e8f6d04172b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to use the gpu for training\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881aa356-2b36-4aae-b0c7-7293947ff120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load model on pre-trained COCO\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    \n",
    "    # Get number of inputs for classifier\n",
    "    in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace pre-trained head with new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "    \n",
    "    # Get input features for the mask classifier\n",
    "    in_feat_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # Replace predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_feat_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a445cc-9f53-49ac-8acb-b7be41373b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    #mean = [0.3014, 0.2905, 0.2335]\n",
    "    #std = [0.0938, 0.0904, 0.0874]\n",
    "    d_transforms = []\n",
    "    d_transforms.append(T.Resize([1125,2000]))\n",
    "    d_transforms.append(T.PILToTensor())\n",
    "    d_transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    #d_transforms.append(transform_Normalize(mean=mean, std=std))\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        d_transforms.append(T.RandomHorizontalFlip())\n",
    "    return T.Compose(d_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38956387-b91c-41a0-a09e-e2a7e6069a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model - using pretrained means it already knows the classification of 1000 classes defined\n",
    "# in the ImageNet database\n",
    "# The model is here chosen to be resnet 18, but might be changed for resnet 50 in the future for a faster R-CNN network.\n",
    "# Note that this current model is NOT an R-CNN, which might mean it's not very good currently since we are looking for\n",
    "# the region(s) of the picture in which the item is located, not a classification of the image as a whole\n",
    "\n",
    "# Remake the transforms, now with normalization using the acquired mean and std\n",
    "train_path = \"../Data/train/\"\n",
    "test_path = \"../Data/test/\"\n",
    "\n",
    "# Make our datasets\n",
    "train_ds = PVDataSet(train_path, get_transform(train=True))\n",
    "test_ds = PVDataSet(test_path, get_transform(train=False))\n",
    "\n",
    "# Put them into loaders\n",
    "# Collate_fn ensures correct data padding\n",
    "# Num_workers speeds up the process by allowing paralell processing\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_ds, batch_size = 16, \n",
    "                                           shuffle = True, collate_fn=utils.collate_fn) #num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_ds, batch_size = 16, \n",
    "                                          shuffle = False, collate_fn=utils.collate_fn) #num_workers=2)\n",
    "\n",
    "# Define how many new classes we would like to learn - here it is only solar panels for the moment\n",
    "num_classes = 2 # Solar panels + background\n",
    "\n",
    "# Prepare matrices for forward propagation using the number of classes and features\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# Set device to GPU if available\n",
    "#device = set_device()\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Define the loss function (between 0.001 - 0.1) & optimizer (Stochastic Gradient Descent)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params, lr = 0.001, momentum = 0.9, weight_decay = 0.003)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07909293-8e57-4e3f-b7aa-34278f850533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\askeo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/161]  eta: 0:40:42  lr: 0.000007  loss: 11.3072 (11.3072)  loss_classifier: 0.6364 (0.6364)  loss_box_reg: 0.0137 (0.0137)  loss_mask: 3.9377 (3.9377)  loss_objectness: 5.7704 (5.7704)  loss_rpn_box_reg: 0.9489 (0.9489)  time: 15.1736  data: 1.4103\n",
      "Epoch: [0]  [ 40/161]  eta: 0:32:12  lr: 0.000257  loss: 1.1206 (3.3780)  loss_classifier: 0.0821 (0.2556)  loss_box_reg: 0.0096 (0.0324)  loss_mask: 0.0013 (0.8845)  loss_objectness: 0.4900 (1.6335)  loss_rpn_box_reg: 0.4634 (0.5720)  time: 15.9145  data: 1.6649\n",
      "Epoch: [0]  [ 80/161]  eta: 0:21:26  lr: 0.000507  loss: 0.9007 (2.1803)  loss_classifier: 0.0599 (0.1661)  loss_box_reg: 0.0076 (0.0272)  loss_mask: 0.0005 (0.4479)  loss_objectness: 0.3569 (1.0245)  loss_rpn_box_reg: 0.4589 (0.5146)  time: 15.7985  data: 1.5821\n",
      "Epoch: [0]  [120/161]  eta: 0:10:51  lr: 0.000756  loss: 0.9020 (1.7663)  loss_classifier: 0.0623 (0.1354)  loss_box_reg: 0.0055 (0.0264)  loss_mask: 0.0003 (0.3000)  loss_objectness: 0.3471 (0.8117)  loss_rpn_box_reg: 0.4449 (0.4928)  time: 15.8528  data: 1.6212\n",
      "Epoch: [0]  [160/161]  eta: 0:00:15  lr: 0.001000  loss: 0.8024 (1.5423)  loss_classifier: 0.0552 (0.1172)  loss_box_reg: 0.0014 (0.0235)  loss_mask: 0.0002 (0.2255)  loss_objectness: 0.3007 (0.6943)  loss_rpn_box_reg: 0.4499 (0.4817)  time: 15.8905  data: 1.6481\n",
      "Epoch: [0] Total time: 0:42:28 (15.8310 s / it)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\engine.py:89\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     86\u001b[0m iou_types \u001b[38;5;241m=\u001b[39m _get_iou_types(model)\n\u001b[0;32m     87\u001b[0m coco_evaluator \u001b[38;5;241m=\u001b[39m CocoEvaluator(coco, iou_types)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m metric_logger\u001b[38;5;241m.\u001b[39mlog_every(data_loader, \u001b[38;5;241m100\u001b[39m, header):\n\u001b[0;32m     90\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32m~\\Documents\\Skole\\Autumn22\\AerialTools\\AerialToolsCV\\ModelTraining\\utils.py:200\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    198\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    199\u001b[0m total_time_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(total_time)))\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Total time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s / it)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Train the nn\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train one epoch, print every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq = 10)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    evaluate(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436dd099-74dc-4b10-a8ae-507c8776307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "img_path = \"../Data/train/img/2021_02_03_10_A_90DJI_0032_height_30m.JPG\"\n",
    "mask_path = \"../Data/train/labels/2021_02_03_10_A_90DJI_0032_height_30m.png\"\n",
    "\n",
    "# Convert the image to RGB and resize\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "w, h = img.size\n",
    "img = img.resize((w//2, h//2))\n",
    "\n",
    "# Convert to grayscale and resize\n",
    "mask = Image.open(mask_path)\n",
    "mask = ImageOps.grayscale(mask)\n",
    "w, h = mask.size\n",
    "mask = mask.resize((w//2, h//2))\n",
    "\n",
    "# Convert PIL to np-array\n",
    "mask = np.array(mask)\n",
    "label_im = label(mask)\n",
    "vals = np.unique(label_im)\n",
    "vals = vals[1:]\n",
    "masks = (label_im[:, None, None] == vals[:, None, None])\n",
    "masks = np.transpose(masks, (2,1,0,3))\n",
    "\n",
    "num_objs = len(vals)\n",
    "boxes = []\n",
    "for i in range(num_objs):\n",
    "    pos = np.where(label_im == vals[i])\n",
    "    xmin = np.min(pos[1])\n",
    "    xmax = np.max(pos[1])\n",
    "    ymin = np.min(pos[0])\n",
    "    ymax = np.max(pos[0])\n",
    "    boxes.append([xmin, ymin, xmax, ymax])\n",
    "print(boxes)\n",
    "\n",
    "\n",
    "# convert everything into a torch.Tensor\n",
    "boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "# there is only one class\n",
    "labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "image_id = torch.tensor(0)\n",
    "area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "# suppose all instances are individual\n",
    "iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "#print(np.shape(masks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
